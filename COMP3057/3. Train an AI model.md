- Knowledge
	- stored in learnable parameters
		- weights
			- How much each input matters
		- bias
			- Extra tweaks to balance output
- Loss Function
	- const funtion or objective function
	- a mathematical function that measures the difference between a model’s prediction and the true target value
- Training Algorithm
	- Training
		- the process of adjusting model parameters so the predictions get better over time
	- It compares the model’s prediction with the correct answer
	- Then changes the parameters to reduce the error between the prediction and groundtruth
- Stochastive Gradient Descent (SGD)
	- learns using one data point at a time
	- gradient
		- compass that points to the direction of improvement
- Define model in PyTorch
```python
class MyLinearModel(torch.nn.Module):
	def __init__(self):
		super(MyLinearModel, self).__init__()
		# define w and b as learnable parameters
		self.w= torch.nn.Parameter(torch.randn(1, requires_grad=True))
		self.b= torch.nn.Parameter(torch.zeros(1, requires_grad=True))
	def forward(self, x):
		return self.w * x + self.b # y = wx+ b
		
# Create model
model = MyLinearModel()
```
- `(torch.nn.Module)`
	- means this class inherits from PyTorch's `nn.Module`
- `torch.nn.Parameter(...)`
	- these are learnable parameters
- `requires_grad=True`
	- means these values can be updated during training (using gradients)
- `forward`
	- a special function in PyTorch
	- defines how the input is transformed into output
	- when calling `model(x)`
		- PyTorch will automatically use `forward` fuction

- How SGD works
	- For each training point
		1. Predict output using current w and b
		2. Measure the error (loss) 
		3. Compute the gradients 
		4. Update w and b to reduce the error 
		5. Repeat! 
	- Stochastic
		- One point at a time
	- How gradient works
		- $\theta = \theta - \eta · gradient$

- Three flavors of gradient descent
	- Batch Gradient Descent
		- Uses all data points for one update
		- very accurate, but very slow
	- Stochastic Gradient Descent
		- Uses one data point per update
		- very fast, but very noisy
	- Mini-Batch Gradient Descent
		- Uses a small batch per update
		- the most practical choice
		- batch
			- a group of training samples processed together in one forward and backward pass
		- Instead of updating model parameters after each sample, we update after each batch

- Use gradient in PyTorch
```Python
Tensor.backward(gradient=None, retain_graph=None, create_graph=False, inputs=None)
```
```python

  criterion = torch.nn.MSELoss()
  optimizer = torch.optim.SGD(model.parameters(), lr=0.1)
  
  # Forward pass
  y_pred = model(x_data)
  loss = criterion(y_pred, y_data)

  # Backwards pass
  optimizer.zero_grad() # clear old gradients
  loss.backward()    # compute new gradients
  optimizer.step()   # update parameters
```
- `criterion`
	- the loss function
- `torch.optim.SGD`
	- works with any batch size
	- gradient is averaged across the batch
	- batch_size = 1
		- pure SGD
	- batch_size > 1
		- mini-batch SGD
- `zero_grad()`
	- erase old gradient values
- `backward()`
	- backpropagation
	- compute how each parameter (w, b) affect the error
- `step()`
	- optimizer updates parameters a little bit using gradients

- Testing
	- Evaluating trained model on new, unseen data
	- Purpose
		- Check if the model learned general rules, not just memorization
	- Training error $\neq$ Real-world performance
	- Testing ensures
		- Generalization ability
		- Robustness
		- Prevention of overfitting

- Training outcome
	- Good fit
		- low training error
		- low testing error
	- Underfitting
		- fails to learn patterns from training data
		- performs poorly on both training and testing
		- Causes
			- model is too simple
			- not trained long enough
	- Overfitting
		- learns training data too well (even noise)
		- Perform poorly on unseen data
		- Causes
			- model is too complex
			- not enough data or regularization (loss function is not fit)

- Learning curve
	- x-axis
		- training steps / epoches
	- y-axis
		- error or loss value
	- two curves
		- training curve
			- error on training set
		- testing curve
			- error on unseen data
	- ideal training case
		- the model is learning effectively
		- the loss decrease consistently
		- no signs of overfitting, instability or stagnation
	- slow decrease
		- too small learning rate
	- loss fluctuates a lot
		- too large learning rate / unstable training
	- Both training & testing losses stay high
		- Underfitting
	- Training loss decrease, testing loss decrease and then increase
		- Overfitting
	- Traning & testing losses both decrease and stay close
		- Good fit
