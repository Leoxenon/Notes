- Model and Prediction
	- $E(Y|x) = µY|x$
	- $µY|x = A+Bx$
	- $Y = A+Bx+ε$
		- Y = Dependentorresponse variable
		- x = Independent or predicator variable
		- ε = Random error component and E(ε) = 0
			- ε ∼ N(0, σ^2)
			- Y ∼ N(A+Bx, σ^2)
		- A = y-intercept of the line
		- B = Slope of theline
- Simple Linear Regression Model
	- $\hat{y} =a+bx$
	- The estimated values of A and B are denoted by a and b respectively
	- Least Square Fitting
		- The deviations, or errors of prediction, are the vertical distances between observed and predicted values
		- sum of squares of the error (SSE)
			- $SSE = ∑ (y_{i} - \hat{y_{i}})^2$
		- residual
			- $ε_{i} = y_{i} - \hat{y_{i}}$
			- $∑ε_{i} = 0$
		- there is one and only one line for which the SSE is a minimum
			- least squares regression line (least squares prediction equation)
	- Least Squares Estimates
		- $b = \frac{SS_{xy}}{SS_{xx}}$
		- $a = \bar{y} - b\bar{x}$
		- $SS_{xy} = ∑(X_{i} - \bar{X})(Y_{i} - \bar{Y}) = ∑x_{i}y_{i} - \frac{∑x_{i}∑y_{i}}{n}$
		- $SS_{xx} = ∑(X_{i} - \bar{X})^2 = ∑x_{i}^2 - \frac{(∑x_{i})^2}{n}$
	- Variance of Error
		- Estimator of $σ^2$
			- $s^2 = \frac{SSE}{n - 2}$
			- $SSE = ∑ (y_{i} - \hat{y_{i}})^2 = SS_{yy} - b SS_{xy}$
			- $SS_{yy} = ∑(Y_{i} - \bar{Y})^2 = ∑y_{i}^2 - \frac{(∑y_{i})^2}{n}$
		- Coefficient of Determination
			- total sum of squares (SST)
				- $SST = ∑ (y_{i} - \bar{y_{i}})^2 = SS_{yy}$
				- represents the error of prediction in the absence of a regression model
			- regression sum of squares (SSR)
				- $SSR = SST−SSE = ∑ (\hat{y_{i}} - \bar{y_{i}})^2$
				- SSR represents the portion of SST that is explained by the regression model
				- SSE represents the portion of SST that is not expained by the regression model
			- $r^2 = \frac{SSR}{SST} = \frac{SST - SSE}{SST} = \frac{bSS_{xy}}{SS_{yy}}$
				- The corresponding population parameter is denoted by $ρ^2$